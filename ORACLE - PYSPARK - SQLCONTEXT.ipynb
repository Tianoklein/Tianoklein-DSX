{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "### TABELA para SPARK DATAFRAME", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from ingest.Connectors import Connectors\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling o149.load.\n: com.ibm.connect.api.SCAPIException: CDICO0100E: Connection failed: [IBM][Oracle JDBC Driver]__HOSTNAME__\n\tat com.ibm.connect.jdbc.AbstractJdbcConnector.driverConnect(AbstractJdbcConnector.java:718)\n\tat com.ibm.connect.jdbc.AbstractJdbcConnector.connect(AbstractJdbcConnector.java:666)\n\tat com.ibm.connect.jdbc.AbstractJdbcConnector.connect(AbstractJdbcConnector.java:168)\n\tat com.ibm.connect.api.DelegatingConnector.connect(DelegatingConnector.java:48)\n\tat com.ibm.connect.api.pool.PooledConnector.connect(PooledConnector.java:76)\n\tat com.ibm.connect.spark.LazyConnector$$anonfun$reconnect$1.apply(ConnectorFactory.scala:42)\n\tat com.ibm.connect.spark.LazyConnector$$anonfun$reconnect$1.apply(ConnectorFactory.scala:38)\n\tat scala.Option.foreach(Option.scala:257)\n\tat com.ibm.connect.spark.LazyConnector.reconnect(ConnectorFactory.scala:38)\n\tat com.ibm.connect.spark.LazyConnector.apply(ConnectorFactory.scala:31)\n\tat com.ibm.connect.spark.LazyConnector$.eval(ConnectorFactory.scala:54)\n\tat com.ibm.connect.spark.ConnectorFactory$.inputInteraction(ConnectorFactory.scala:93)\n\tat com.ibm.connect.spark.ConnectorReader$.rowDefinition(ConnectorReader.scala:24)\n\tat com.ibm.connect.spark.ConnectorRelation$$anonfun$schema$1.apply(ConnectorRelation.scala:26)\n\tat com.ibm.connect.spark.ConnectorRelation$$anonfun$schema$1.apply(ConnectorRelation.scala:19)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.ibm.connect.spark.ConnectorRelation.schema(ConnectorRelation.scala:19)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:40)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:397)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat com.ibm.spark.discover.GenericRelation.schema(GenericRelation.scala:54)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:40)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:397)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-2-dace2abdc813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                      \u001b[1;31m#Connectors.Oracle.SOURCE_SELECT_STATEMENT : sqlcmd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                     }\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mOracleDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.ibm.spark.discover\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mOracleloadOptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mOracleDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mOracleDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o149.load.\n: com.ibm.connect.api.SCAPIException: CDICO0100E: Connection failed: [IBM][Oracle JDBC Driver]__HOSTNAME__\n\tat com.ibm.connect.jdbc.AbstractJdbcConnector.driverConnect(AbstractJdbcConnector.java:718)\n\tat com.ibm.connect.jdbc.AbstractJdbcConnector.connect(AbstractJdbcConnector.java:666)\n\tat com.ibm.connect.jdbc.AbstractJdbcConnector.connect(AbstractJdbcConnector.java:168)\n\tat com.ibm.connect.api.DelegatingConnector.connect(DelegatingConnector.java:48)\n\tat com.ibm.connect.api.pool.PooledConnector.connect(PooledConnector.java:76)\n\tat com.ibm.connect.spark.LazyConnector$$anonfun$reconnect$1.apply(ConnectorFactory.scala:42)\n\tat com.ibm.connect.spark.LazyConnector$$anonfun$reconnect$1.apply(ConnectorFactory.scala:38)\n\tat scala.Option.foreach(Option.scala:257)\n\tat com.ibm.connect.spark.LazyConnector.reconnect(ConnectorFactory.scala:38)\n\tat com.ibm.connect.spark.LazyConnector.apply(ConnectorFactory.scala:31)\n\tat com.ibm.connect.spark.LazyConnector$.eval(ConnectorFactory.scala:54)\n\tat com.ibm.connect.spark.ConnectorFactory$.inputInteraction(ConnectorFactory.scala:93)\n\tat com.ibm.connect.spark.ConnectorReader$.rowDefinition(ConnectorReader.scala:24)\n\tat com.ibm.connect.spark.ConnectorRelation$$anonfun$schema$1.apply(ConnectorRelation.scala:26)\n\tat com.ibm.connect.spark.ConnectorRelation$$anonfun$schema$1.apply(ConnectorRelation.scala:19)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.ibm.connect.spark.ConnectorRelation.schema(ConnectorRelation.scala:19)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:40)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:397)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat com.ibm.spark.discover.GenericRelation.schema(GenericRelation.scala:54)\n\tat org.apache.spark.sql.execution.datasources.LogicalRelation.<init>(LogicalRelation.scala:40)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:397)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "#sqlcmd = \"select * from HELP\"\nOracleloadOptions = {\n                     Connectors.Oracle.HOST              : '__HOSTNAME__',\n                     Connectors.Oracle.PORT              : '1521',\n                     Connectors.Oracle.SID               : 'ORCL',\n                     #Connectors.Oracle.SERVICE_NAME      : '***********',\n                     Connectors.Oracle.USERNAME          : '__DBUSER__',\n                     Connectors.Oracle.PASSWORD          : '__DBPASSWORD__',\n                     Connectors.Oracle.SOURCE_TABLE_NAME         : '__DBTABLE__'\n                     #Connectors.Oracle.SOURCE_SELECT_STATEMENT : sqlcmd \n                    }\nOracleDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**OracleloadOptions).load()\nOracleDF.printSchema()\nOracleDF.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "OracleDF.count()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "OracleDF.PrintSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "OracleDF.write.mode(\"overwrite\").saveAsTable(\"OracleDF\") #\"append\"\n#OracleDF.registerTempTable(\"OracleDFTemp\")"
        }, 
        {
            "source": "### SPARK DATAFRAME para TABELA\n    https://datascience.ibm.com/docs/content/analyze-data/python_load.html", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from ingest.Connectors import Connectors\nOraclesaveoption = {\n                     Connectors.Oracle.HOST              : '__HOSTNAME__',\n                     Connectors.Oracle.PORT              : '1521',\n                     Connectors.Oracle.SID               : 'ORCL',\n                     #Connectors.Oracle.SERVICE_NAME      : '***********',\n                     Connectors.Oracle.USERNAME          : '__DBUSER__',\n                     Connectors.Oracle.PASSWORD          : '__DBPASSWORD',\n                     Connectors.Oracle.TARGET_TABLE_NAME : '__DBTABLE'\n                     #Connectors.Oracle.TARGET_CREATE_STATEMENT : \"INSERT INTO teste1 (nome, sobrenome) VALUES ('33', '3333')\"\n                     #Connectors.Oracle.TARGET_TABLE_ACTION : 'merge'\n                        }"
        }, 
        {
            "source": "##### Criar um DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "data2 = sqlContext.sql(\"select 'RRRRRX' as NOME,\\\n                              'ZZZZZZX' as SOBRENOME\")"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "pyspark.sql.dataframe.DataFrame"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "type(data2)"
        }, 
        {
            "source": "##### .Salvar no Oracle", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "NewOracleDF = data2.write.format(\"com.ibm.spark.discover\").options(**Oraclesaveoption).save()"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}